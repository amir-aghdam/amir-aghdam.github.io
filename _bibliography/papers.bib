@misc{aghdam2025actalignzeroshotfinegrainedvideo,
      bibtex_show={true},
      title={ActAlign: Zero-Shot Fine-Grained Video Classification via Language-Guided Sequence Alignment},
      author={Aghdam, Amir and Hu, Vincent Tao},
      abstract={We address the task of zero-shot fine-grained video classification, where no video examples or temporal annotations are available for unseen action classes. While contrastive vision-language models such as SigLIP demonstrate strong open-set recognition via mean-pooled image-text similarity, they fail to capture the temporal structure critical for distinguishing fine-grained activities. We introduce ActAlign, a zero-shot framework that formulates video classification as sequence alignment. For each class, a large language model generates an ordered sub-action sequence, which is aligned with video frames using Dynamic Time Warping (DTW) in a shared embedding space. Without any video-text supervision or fine-tuning, ActAlign achieves 30.5% accuracy on the extremely challenging ActionAtlas benchmark, where human accuracy is only 61.6%. ActAlign outperforms billion-parameter video-language models while using approximately 8x less parameters. These results demonstrate that structured language priors, combined with classical alignment techniques, offer a scalable and general approach to unlocking the open-set recognition potential of vision-language models for fine-grained video understanding.},
      year={2025},
      eprint={2506.22967},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      selected={true},
      pdf={https://arxiv.org/pdf/2506.22967.pdf},
      html={https://arxiv.org/abs/2506.22967}, 
      preview={ActAlign.png},
}