<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://amir-aghdam.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://amir-aghdam.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-06T03:25:15+00:00</updated><id>https://amir-aghdam.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">The Age of Prompting: Why Every Engineer Needs to Rethink Software</title><link href="https://amir-aghdam.github.io/blog/2025/the-age-of-prompting-why-every-engineer-needs-to-rethink-software/" rel="alternate" type="text/html" title="The Age of Prompting: Why Every Engineer Needs to Rethink Software"/><published>2025-06-20T00:14:58+00:00</published><updated>2025-06-20T00:14:58+00:00</updated><id>https://amir-aghdam.github.io/blog/2025/the-age-of-prompting-why-every-engineer-needs-to-rethink-software</id><content type="html" xml:base="https://amir-aghdam.github.io/blog/2025/the-age-of-prompting-why-every-engineer-needs-to-rethink-software/"><![CDATA[<blockquote>We’re not just coding anymore — we’re prompting, orchestrating, and building alongside AI.</blockquote> <blockquote><em>— Andrej Karpathy</em></blockquote> <p>Andrej Karpathy, former Director of AI at Tesla and founding member of OpenAI, recently gave a powerful talk titled <strong>“Software in the Era of AI.”</strong> While it’s worth watching in full (link below), here’s a distilled, structured guide to the <strong>core ideas and takeaways</strong> — especially for ML engineers building at the frontier.</p> <p>🎥 <strong>Watch the full talk here →</strong> <a href="https://www.youtube.com/watch?v=LCEmiRjPEtQ">https://www.youtube.com/watch?v=LCEmiRjPEtQ</a></p> <h3>🛡 The Three Generations of Software</h3> <p>Karpathy introduces a new framework to understand how <strong>software is evolving:</strong></p> <h3>🔹 Software 1.0 — Code as Instructions</h3> <ul><li>Traditional software: humans write code in languages like Python, C++, etc.</li><li>Think: explicit logic, if-statements, loops.</li></ul> <h3>🔹 Software 2.0 — Neural Nets as Programs</h3> <ul><li>Instead of writing logic, you <strong>train models</strong> using data and optimization.</li><li>The “program” is now a set of learned weights.</li></ul> <h3>🔹 Software 3.0 — Prompts as Programs</h3> <ul><li>You don’t write code or train models — you <strong>prompt</strong> a large language model (LLM).</li><li>Programming is now in <strong>natural language</strong> like English.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*TNFzGhLwv9x5DjHsoN-zSg.png"/><figcaption>Karpathy’s framing of Software 1.0 (manual code), 2.0 (neural networks), and 3.0 (prompting LLMs in natural language).</figcaption></figure> <h3>💡 Prompting is Programming</h3> <p>In the Software 3.0 world, <strong>your prompt becomes the program.</strong></p> <p>Example:<br/> To classify sentiment, you can:</p> <ul><li>Write custom code (1.0)</li><li>Train a classifier (2.0)</li><li>Prompt an LLM: <em>“Classify the following review as positive or negative…”</em> (3.0)</li></ul> <p>This shift is not just about convenience — it’s a <strong>new computing paradigm</strong>.</p> <h3>💻 LLMs as Operating Systems</h3> <p>Karpathy argues that LLMs aren’t just tools — they’re becoming <strong>complex software platforms</strong>, like operating systems.</p> <h3>Similarities to OS:</h3> <ul><li>LLMs orchestrate memory (context windows), compute (token-by-token inference), and I/O (tool use).</li><li>Closed-source models (GPT, Gemini, Claude) resemble Windows/macOS.</li><li>Open-source models (LLaMA, Mistral) are like Linux.</li><li>LLM-native apps like <strong>Cursor</strong> or <strong>Perplexity</strong> run <em>on top</em> of this OS layer.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cgkv0ULRZK5GcZv3w1Gmlg.png"/><figcaption>LLMs acting as new operating systems — orchestrating memory (context), compute (token inference), and I/O (tool use).</figcaption></figure> <h3>🛠 LLM Apps Are Partially Autonomous Systems</h3> <p>Karpathy emphasizes that <strong>the most useful AI applications today aren’t full agents</strong> — they’re <strong>partially autonomous tools</strong>.</p> <h3>Example:</h3> <p><strong>Cursor</strong> is an AI-powered code editor:</p> <ul><li>You can type manually (human control).</li><li>Or you can highlight code and let the AI rewrite it.</li><li>Or let it modify an entire repo (full autonomy).</li></ul> <p>💡 This creates an <strong>“autonomy slider”</strong>: control how much work you give the AI.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*n7Lc5u2M4tGbbUc-Z2boHg.png"/><figcaption>LLM-native apps like Perplexity combine AI logic with familiar GUI controls to keep humans in the loop.</figcaption></figure> <h3>🧐 LLMs Are “People Spirits”</h3> <p>Karpathy offers a provocative analogy: LLMs are <strong>“people spirits”</strong> — stochastic simulations of humans with memory, reasoning, and personality.</p> <h3>LLM strengths:</h3> <ul><li>Huge general knowledge</li><li>Superhuman pattern recognition</li></ul> <h3>But also weaknesses:</h3> <ul><li>Hallucinations</li><li>No persistent memory</li><li>Easily manipulated (prompt injections)</li></ul> <p>This makes working with LLMs a <strong>human-AI cooperation game</strong>, where:</p> <ul><li>AI generates</li><li>Human verifies</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*93dkBZkOy_8gjpEIKOhH4Q.png"/><figcaption>Karpathy’s human-AI collaboration loop: AI generates; humans verify — and the faster this loop, the better.</figcaption></figure> <h3>🧰 How to Build Great LLM Apps</h3> <p>According to Karpathy, effective LLM apps share 4 common features:</p> <h3>1. Context management</h3> <p>Apps feed LLMs the right info at the right time (e.g. embeddings of your codebase).</p> <h3>2. Multi-LLM orchestration</h3> <p>Use different models for different jobs (chat, retrieval, diffs).</p> <h3>3. Custom GUI for audit &amp; control</h3> <p>A good interface lets users <em>see what the AI is doing</em> and approve/reject outputs quickly.</p> <h3>4. Autonomy slider</h3> <p>Let users control how much power the AI gets — from autocomplete to repo-wide edits.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*O4QpeNDOo4RC7JC_jdj2iw.png"/><figcaption>Cursor allows developers to slide between manual coding and full AI-driven repo changes — a spectrum of autonomy.</figcaption></figure> <h3>🧠 Design for Speed: Generation + Verification</h3> <p>Karpathy says: <strong>we’re no longer just writing software — we’re verifying AI-generated software.</strong></p> <h3>How to speed up the feedback loop:</h3> <ul><li>Use visual GUIs to inspect results (faster than reading raw text).</li><li>Write <strong>clear, constrained prompts</strong> to reduce failures.</li><li>Avoid mega-diffs; think in small chunks.</li></ul> <h3>🌍 Build for LLMs, Not Just Humans</h3> <p>A surprising insight: LLMs are now <strong>users of software</strong>. Just like humans or APIs.</p> <h3>What this means:</h3> <ul><li>Write docs in <strong>LLM-readable formats</strong> (e.g. markdown, JSON).</li><li>Avoid instructions like “click here” — replace with <strong>API calls or shell commands</strong>.</li><li>Add lm.txt files to help LLMs understand your site’s purpose.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*8psbDHTxGMjC0V9CH23C5A.png"/><figcaption>Designing for agents: Simplified, machine-readable documentation helps LLMs understand and interact with your software.</figcaption></figure> <h3>✨ Vibe Coding: Everyone’s a Programmer Now</h3> <p>A viral moment from the talk: Karpathy’s coined term <strong>“vibe coding.”</strong></p> <blockquote><em>You don’t know Swift? Doesn’t matter.<br/> Prompt the LLM, copy-paste, tweak, repeat.</em></blockquote> <p>He built working iOS and web apps without knowing the languages, just by <strong>“vibing” with the LLM</strong>.</p> <p>This changes who can build software — and how fast they can do it.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-ChoWCNWZCdfhFdmFEPaIg.png"/><figcaption>Karpathy’s Menu.app — built by ‘vibe coding’ an AI prototype without knowing Swift. The future of accessible dev.</figcaption></figure> <h3>⚧ DevOps is Now the Bottleneck</h3> <p>Ironically, the hardest part isn’t coding — it’s all the <strong>non-code setup</strong>:</p> <ul><li>Auth</li><li>Hosting</li><li>Billing</li><li>Deployment</li></ul> <p>These tasks are still GUI-based and require human clicks. Karpathy asks:</p> <blockquote><em>“Why am I doing this? Let the agents do it!”</em></blockquote> <p>Early attempts of addressing this issue is creating the <strong>Model Context Protocol (MCP)</strong>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yF1VZC1M1vJDT62v1JpfnA.png"/><figcaption>The rise of agents</figcaption></figure> <h3>📜 Takeaways for ML Engineers</h3> <p>✅ Learn to work with prompts, not just code<br/>✅ Develop effective apps by combining GUI + autonomy sliders to keep AI on a leash<br/>✅ Structure apps around fast generate–verify loops<br/>✅ Build documentation and UIs that speak to <strong>agents</strong></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9c861fe9d60b" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>